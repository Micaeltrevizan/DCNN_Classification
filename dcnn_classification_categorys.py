# -*- coding: utf-8 -*-
"""DCNN_CLASSIFICATION CATEGORYS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m9XQT7wQwVT_MdhoHjlKEJh_mesDDn2v

# Bibliotecas
"""

import numpy as np
import math
import re
import pandas as pd
from bs4 import BeautifulSoup
import random
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import drive

!pip install tensorflow==2.8.0

!pip install bert-for-tf2

!pip install sentencepiece

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow_hub as hub
import tensorflow as tf
from tensorflow.keras import layers
import bert

"""# Base de dados#"""

base1 = pd.read_csv('/content/CLMLC140.csv')

base1.info()

#base1.dropna(inplace=True)

VFG = base1[['VFG']]
VFG

print(VFG['VFG'].unique())

base2 = base1[['Customer Comments']]

base = pd.concat([VFG, base2] , axis = 1)

base.dropna(inplace = True)

base

"""# Limpesa das frases"""

base.columns = ['VFG', 'phrase']

base.dropna(inplace=True)

base

def clean_phrase(phrase):
  phrase = BeautifulSoup(phrase, 'lxml').get_text()
  phrase = re.sub(r"@[A-Za-z0-9]+", ' ', phrase)
  phrase = re.sub(r"https?://[A-Za-z0-9./]+", ' ', phrase)
  phrase = re.sub(r"[^a-zA-Z.!?']", ' ', phrase)
  phrase = re.sub(r" +", ' ', phrase)
  return phrase

data_clean = [clean_phrase(phrase) for phrase in base.phrase]

data_clean[0:4]

data_labels = base.VFG.values
data_labels

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
data_labels = encoder.fit_transform(data_labels.reshape(-1, 1))

data_labels

"""# Tokenização"""

FullTokenizer = bert.bert_tokenization.FullTokenizer
bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1', trainable=False)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = FullTokenizer(vocab_file, do_lower_case)

# Função para converter strings em ID
def encode_sentence(sent):
  return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))

data_inputs = [encode_sentence(sentence) for sentence in data_clean]

data_inputs[1]

"""# Criação da base

"""

data_with_len = [[sent, data_labels[i], len(sent)]
                 for i, sent in enumerate(data_inputs)]

data_with_len[0:2]

random.shuffle(data_with_len)
data_with_len.sort(key=lambda x: x[2])
sorted_all = [(sent_lab[0], sent_lab[1])
              for sent_lab in data_with_len if sent_lab[2] > 0]

sorted_all[0:5]

all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,
                                             output_types = (tf.int32, tf.int32))

next(iter(all_dataset))

BATCH_SIZE = 1
all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))

next(iter(all_batched))

len(sorted_all)

NB_BATCHES = len(sorted_all) // BATCH_SIZE
NB_BATCHES

NB_BATCHES_TEST = NB_BATCHES // 10
NB_BATCHES_TEST

all_batched.shuffle(NB_BATCHES)
test_dataset = all_batched.take(NB_BATCHES_TEST)
train_dataset = all_batched.skip(NB_BATCHES_TEST)

"""# Construção do modelo (CNN)"""

class DCNN(tf.keras.Model):

  def __init__(self,
               vocab_size,
               emb_dim=128,
               nb_filters = 50,
               FFN_units=512,
               nb_classes=2,
               dropout_rate=0.1,
               training=False,
               name="dcnn"):
    super(DCNN, self).__init__(name=name)

    self.embedding = layers.Embedding(vocab_size, emb_dim)

    self.bigram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 2,
                                padding='valid',
                                activation='relu')
    self.trigram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 3,
                                padding='valid',
                                activation='relu')
    self.fourgram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 4,
                                padding='valid',
                                activation='relu')

    self.pool = layers.GlobalMaxPool1D()

    self.dense_1 = layers.Dense(units = FFN_units, activation='relu')
    self.dropout = layers.Dropout(rate=dropout_rate)
    if nb_classes == 2:
      self.last_dense = layers.Dense(units=1, activation='sigmoid')
    else:
      self.last_dense = layers.Dense(units=nb_classes, activation='softmax')

  def call(self, inputs, training):
    x = self.embedding(inputs)
    x_1 = self.bigram(x)
    x_1 = self.pool(x_1)
    x_2 = self.trigram(x)
    x_2 = self.pool(x_2)
    x_3 = self.fourgram(x)
    x_3 = self.pool(x_3)

    merged = tf.concat([x_1, x_2, x_3], axis = -1)
    merged = self.dense_1(merged)
    merged = self.dropout(merged, training)
    output = self.last_dense(merged)

    return output

"""# Treinamento"""

VOCAB_SIZE = len(tokenizer.vocab)
EMB_DIM = 100
NB_FILTERS = 100
FFN_UNITS = 256
NB_CLASSES = 10
DROPOUT_RATE = 0.2
NB_EPOCHS = 4

Dcnn = DCNN(vocab_size=VOCAB_SIZE,
            emb_dim=EMB_DIM,
            nb_filters = NB_FILTERS,
            FFN_units = FFN_UNITS,
            nb_classes = NB_CLASSES,
            dropout_rate = DROPOUT_RATE)

if NB_CLASSES == 2:
  Dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
else:
  Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])

checkpoint_path = '/content/drive/MyDrive/Testes FORD/EXPLORER/Explorer/CNN'

ckpt = tf.train.Checkpoint(Dcnn=Dcnn)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)

if ckpt_manager.latest_checkpoint:
  ckpt.restore(ckpt_manager.latest_checkpoint)
  print('Latest checkpoint restored!')

class MyCustomCallBack(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    ckpt_manager.save()
    print("Checkpoint savet at {}".format(checkpoint_path))

history = Dcnn.fit(train_dataset,
                   epochs=NB_EPOCHS,
                   #steps_per_epoch = 20000 ,
                   callbacks=[MyCustomCallBack()])

"""# Avaliação"""

history.history.keys()

plt.plot(history.history['loss'])
plt.title('Loss progress');

#results = Dcnn.evaluate(test_dataset)
#print(results)

def get_prediction(sentence):
    tokens = encode_sentence(sentence)
    inputs = tf.expand_dims(tokens, 0)
    output = Dcnn(inputs, training=False).numpy()
    output = np.argmax(output, -1)
    category = encoder.inverse_transform(output)
    return category

"""def get_prediction_a(sentence):
  tokens = encode_sentence(sentence)
  inputs = tf.expand_dims(tokens, 0)  #(batch_size) (1,...)

  output = Dcnn(inputs, training=False)
  print(output)

def get_prediction_b(sentence):
  tokens = encode_sentence(sentence)
  inputs = tf.expand_dims(tokens, 0) # (batch_size) (1,...)

  output = Dcnn(inputs, training=False)

  sentiment = math.floor(output*2)

  if sentiment == 0:
    print('Negativo')
  elif sentiment == 1:
    print('Positivo')

def get_prediction_c(sentence):
  tokens = encode_sentence(sentence)
  inputs = tf.expand_dims(tokens, 0) # (batch_size) (1,...)

  output = Dcnn(inputs, training=False)

  sentiment = math.floor(output*2)

  if sentiment == 0:
    return('Negative')
  elif sentiment == 2:
    return('Positive')
  elif sentiment == 1:
    return('Neutro')
"""

get_prediction('I was very disappointed to lose my front camera on this vehicle. Ive had that for so long it has been difficult getting used to not having it.')

#get_prediction_a((clean_phrase('My drivers seat seems a little loose on hard curves and I feel the rims are “soft”.')))

"""# Analisando uma base de dados com o modelo de predição"""

phrase_cars = pd.read_csv('/content/F9 - Copy.csv',encoding='latin-1')
phrase_cars1 = phrase_cars[['Technician Comments']]
#phrase_cars1 = phrase_cars1[phrase_cars['QNPS Clarifier (Binned) Desc']== 'FRONT CAMERA']
#phrase_cars1 = phrase_cars1[phrase_cars['QNPS Sentiment (Binned)']== -1]
phrase_cars = phrase_cars1[['Technician Comments']]

phrase_cars

phrase_cars.dropna(inplace = True)

phrase_cars

phrase_cars1 = phrase_cars.astype(str)

phrase_cars1.columns = ['phrase']

texts = phrase_cars1['phrase'].tolist()

#texts = phrase_cars

#texts.remove("Fuel mileage.")

df = pd.DataFrame({'phrase': texts})
df['VFG'] = df['phrase'].map(get_prediction)
print(df)

df = df.astype(str)

df = pd.DataFrame(df, columns = ['phrase', 'Tag'])

sns.countplot(x = df['Tag'])

#sns.countplot(x = phrase_cars1['phrase_polarity'])

#df0 = pd.read_csv('/content/JDPower_2021.csv')
#pd.concat([df0, df])

#df0

"""Pipeline de limpeza dos textos

# Baixar dados
"""

from google.colab import files

#ex = pd.read_excel('/content/QNPS_OBS.xlsm')
#eqp = pd.read_excel('/content/Equipment Group_21-22-23.xlsx')

#left = pd.merge(eqp,ex, left_on =['QNPS VIN'], right_on=['QNPS VIN'], how='left')

df.to_excel('teste.xlsx')
files.download('teste.xlsx')